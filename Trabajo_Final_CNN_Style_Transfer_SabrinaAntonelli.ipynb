{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCY6UbkkI9_N"
   },
   "source": [
    "# Style Transfer\n",
    "\n",
    "<img src=\"https://i0.wp.com/chelseatroy.com/wp-content/uploads/2018/12/neural_style_transfer.png?resize=768%2C311&ssl=1\">\n",
    "\n",
    "La idea de este trabajo final es reproducir el siguiente paper:\n",
    "\n",
    "https://arxiv.org/pdf/1508.06576.pdf\n",
    "\n",
    "El objetivo es transferir el estilo de una imagen dada a otra imagen distinta. \n",
    "\n",
    "Como hemos visto en clase, las primeras capas de una red convolucional se activan ante la presencia de ciertos patrones vinculados a detalles muy pequeños.\n",
    "\n",
    "A medida que avanzamos en las distintas capas de una red neuronal convolucional, los filtros se van activando a medida que detectan patrones de formas cada vez mas complejos.\n",
    "\n",
    "Lo que propone este paper es asignarle a la activación de las primeras capas de una red neuronal convolucional (por ejemplo VGG19) la definición del estilo y a la activación de las últimas capas de la red neuronal convolucional, la definición del contenido.\n",
    "\n",
    "La idea de este paper es, a partir de dos imágenes (una que aporte el estilo y otra que aporte el contenido) analizar cómo es la activación de las primeras capas para la imagen que aporta el estilo y cómo es la activación de las últimas capas de la red convolucional para la imagen que aporta el contenido. A partir de esto se intentará sintetizar una imagen que active los filtros de las primeras capas que se activaron con la imagen que aporta el estilo y los filtros de las últimas capas que se activaron con la imagen que aporta el contenido.\n",
    "\n",
    "A este procedimiento se lo denomina neural style transfer.\n",
    "\n",
    "# En este trabajo se deberá leer el paper mencionado y en base a ello, entender la implementación que se muestra a continuación y contestar preguntas sobre la misma.\n",
    "\n",
    "# Una metodología posible es hacer una lectura rápida del paper (aunque esto signifique no entender algunos detalles del mismo) y luego ir analizando el código y respondiendo las preguntas. A medida que se planteen las preguntas, volviendo a leer secciones específicas del paper terminará de entender los detalles que pudieran haber quedado pendientes.\n",
    "\n",
    "Lo primero que haremos es cargar dos imágenes, una que aporte el estilo y otra que aporte el contenido. A tal fin utilizaremos imágenes disponibles en la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "kyHsa2t0SxZi",
    "outputId": "e72fcf52-62ed-42f1-f64e-cdb05d049797"
   },
   "outputs": [],
   "source": [
    "# Imagen para estilo\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/5/52/La_noche_estrellada1.jpg\n",
    "\n",
    "# Imagen para contenido\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Neckarfront_T%C3%BCbingen_Mai_2017.jpg/775px-Neckarfront_T%C3%BCbingen_Mai_2017.jpg\n",
    "\n",
    "# Creamos el directorio para los archivos de salida\n",
    "!mkdir /content/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "id": "NIxH20o2eFoc",
    "outputId": "4785bcbb-4070-4e68-c2b5-4a1dfdccbad2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "iLkV1bnFl_tK"
   },
   "outputs": [],
   "source": [
    "# Definimos las imagenes que vamos a utilizar, y el directorio de salida\n",
    "\n",
    "base_image_path = Path(\"./content/Neckarfront_Tübingen_Mai_2017.jpeg\")\n",
    "style_reference_image_path = Path(\"./content/La_noche_estrellada1.jpeg\")\n",
    "result_prefix = Path(\"./content/output8\")\n",
    "\n",
    "#respuesta 9\n",
    "#base_image_path = Path(\"./content9/IMG_0304.jpeg\")\n",
    "#style_reference_image_path = Path(\"./content9/style.jpeg\")\n",
    "#result_prefix = Path(\"./content9/output9\")\n",
    "\n",
    "\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz2PeGfpeYzj"
   },
   "source": [
    "# 1) En base a lo visto en el paper ¿Qué significan los parámetros definidos en la siguiente celda?\n",
    "\n",
    "Respuesta: Ajustando estos parametros podemos establecer un balance entre la rentención del contenido, el estilo de transferencia y la reduccion del ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "P9Dt3aaEmJWS"
   },
   "outputs": [],
   "source": [
    "total_variation_weight = 0.5\n",
    "style_weight = 0.5\n",
    "content_weight = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "CQQJOhCVuse6"
   },
   "outputs": [],
   "source": [
    "# Definimos el tamaño de las imágenes a utilizar\n",
    "width, height = load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg2ct-8agm1E"
   },
   "source": [
    "# 2) Explicar qué hace la siguiente celda. En especial las últimas dos líneas de la función antes del return. ¿Por qué?\n",
    "\n",
    "Ayuda: https://keras.io/applications/\n",
    "\n",
    "Respuesta: \n",
    "- np.expand_dims(img, axis=0) -> expande el array en 1 dimension para poder procesar en batch.\n",
    "- vgg19.preprocess_input(img) -> Las imágenes se convierten de RGB a BGR, luego cada canal de color se centra en cero con respecto al conjunto de datos de ImageNet, sin escala. Los valores utilizados para centrar en 0 cada canal son [103.939, 116.779, 123.68] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "tAkljg4zuzYd"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTf0YDSagt10"
   },
   "source": [
    "# 3) Habiendo comprendido lo que hace la celda anterior, explique de manera muy concisa qué hace la siguiente celda. ¿Qué relación tiene con la celda anterior?\n",
    "\n",
    "Respuesta: Se convierte la imagen al formato original BGR -> RGB. \n",
    "Los mismos valores medios ([103.939, 116.779, 123.68]) utilizados en la funcion preprocess_input() se agregan a cada canal respectivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "y5LaTrsAu14z"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "HYNio09mu4S3"
   },
   "outputs": [],
   "source": [
    "# get tensor representations of our images\n",
    "# K.variable convierte un numpy array en un tensor, para \n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "a1Lbw02Uu--o",
    "outputId": "6cc926fa-55af-43fa-fe91-3b68c0910502"
   },
   "outputs": [],
   "source": [
    "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJEi0YI3Uzrm"
   },
   "source": [
    "Aclaración:\n",
    "\n",
    "La siguiente celda sirve para procesar las tres imagenes (contenido, estilo y salida) en un solo batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "gGO_jGFfvEbF"
   },
   "outputs": [],
   "source": [
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "tdG59VRavHGB",
    "outputId": "a133befb-68d1-4c51-99e6-417c1103f726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG19 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70-vs_jZkKVc"
   },
   "source": [
    "# 4) En la siguientes celdas:\n",
    "\n",
    "- ¿Qué es la matriz de Gram?¿Para qué se usa?\n",
    "La matriz gram representa la correlación de las features de estilo de los canales. La altura y ancho de la matriz son ambos el numero de canales. Usamos esta matriz de Gram para representar el style output de cualquier style layer. \n",
    "- ¿Por qué se permutan las dimensiones de x?\n",
    "Porque es la manera computacional de ejecutar la Gram Matrix y encontrar aquellos features que se activan simultaneamente en una misma posicion. Primero se permuta para que de bien luego al transponer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K1FODPATvJ1k"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBQkKFY0Rbx-"
   },
   "source": [
    "# 5) Losses:\n",
    "\n",
    "Explicar qué mide cada una de las losses en las siguientes tres celdas.\n",
    "\n",
    "Rta: \n",
    "\n",
    "- Content_loss: mide la diferencia en las features del contenido entre la imagen sintetizada y la content image a través de la función squared loss.\n",
    "- Style loss: Utiliza la funcion squared loss para medir la diferencia de estilo entre la imagen sintetizada y la imagen de estilo.\n",
    "- Total variation loss: mide el ruido de la imagen sintetizada calculando la diferencia de frecuencia de un pixel con sus vecinos. Se espera que la transicion sea lo mas suavizada posible.\n",
    "\n",
    "https://d2l.ai/chapter_computer-vision/neural-style.html#defining-the-loss-function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "1-Gt0ahWvN6q"
   },
   "outputs": [],
   "source": [
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "XCqnju5RvQCo"
   },
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "udEp5h31vRnY"
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    a = K.square(\n",
    "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "-65vcinbvTZ0"
   },
   "outputs": [],
   "source": [
    "# Armamos la loss total\n",
    "loss = K.variable(0.0)\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss = loss + content_weight * content_loss(base_image_features,\n",
    "                                            combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :] \n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss = loss + (style_weight / len(feature_layers)) * sl\n",
    "loss = loss + total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "pbz4n1OhvV2K",
    "outputId": "c2b208c6-7ddd-4a40-eeda-525f0809b963"
   },
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "f_outputs = K.function([combination_image], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JbydbOaVcvU"
   },
   "source": [
    "# 6) Explique el propósito de las siguientes tres celdas. ¿Qué hace la función fmin_l_bfgs_b? ¿En qué se diferencia con la implementación del paper? ¿Se puede utilizar alguna alternativa?\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "- 1: Es una funcion de evaluacion de loss y el gadiente de la loss de la imagen generada.\n",
    "- 2: Se define una clase Evaluator para acceder a la funcion de loss y gradiente en forma separada para poder ser utilizada por el optimizador (scipy-based).\n",
    "- 3: Ejecuta la funcion de optimizacion en n iteraciones. Primero preprocesa la imagen base y luego itera ejecutando el optimizador para reducir la loss y guardando la imagen generada.\n",
    "\n",
    "- fmin_l_bfgs_b ejecuta la optimizacion l-BFGS. A diferencia de gradient descent, es un optimizador mucho mas rapido ya que almacena pocos vectores a partir de los que reconstruye las matrices.\n",
    "\n",
    "https://yashk2810.github.io/Neural-Style-Transfer/\n",
    "\n",
    "https://www.quora.com/What-is-an-intuitive-explanation-of-BFGS-and-limited-memory-BFGS-optimization-algorithms\n",
    "\n",
    "- El paper solo usa las loss de style y content, mientras el ejemplo que estamos viendo usa ademas el total loss.\n",
    "- Gradient descent puede ser un metodo alternativo pero no seria lo correcto desde el punto de vista del tiempo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "zVE1_qemvZeN"
   },
   "outputs": [],
   "source": [
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "# this Evaluator class makes it possible\n",
    "# to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions,\n",
    "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "Qbl9roIgvdb1"
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb0yOEl-WOE6"
   },
   "source": [
    "# 7) Ejecute la siguiente celda y observe las imágenes de salida en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n31YBwCVvhAI",
    "outputId": "4c1bf03c-9d66-48ea-93f2-4489fc20beaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 4941692400.0\n",
      "Image saved as content/output8/output_at_iteration_0.png\n",
      "Iteration 0 completed in 220s\n",
      "Start of iteration 1\n",
      "Current loss value: 3253998000.0\n",
      "Image saved as content/output8/output_at_iteration_1.png\n",
      "Iteration 1 completed in 2728s\n",
      "Start of iteration 2\n",
      "Current loss value: 2774243800.0\n",
      "Image saved as content/output8/output_at_iteration_2.png\n",
      "Iteration 2 completed in 220s\n",
      "Start of iteration 3\n",
      "Current loss value: 2554271200.0\n",
      "Image saved as content/output8/output_at_iteration_3.png\n",
      "Iteration 3 completed in 215s\n",
      "Start of iteration 4\n",
      "Current loss value: 2417124400.0\n",
      "Image saved as content/output8/output_at_iteration_4.png\n",
      "Iteration 4 completed in 242s\n",
      "Start of iteration 5\n",
      "Current loss value: 2315459800.0\n",
      "Image saved as content/output8/output_at_iteration_5.png\n",
      "Iteration 5 completed in 238s\n",
      "Start of iteration 6\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "\n",
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "x = preprocess_image(base_image_path)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "    fname = result_prefix / ('output_at_iteration_%d.png' % i)\n",
    "    save_img(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkiJtofbWWy1"
   },
   "source": [
    "# 8) Generar imágenes para distintas combinaciones de pesos de las losses. Explicar las diferencias. (Adjuntar las imágenes generadas como archivos separados.)\n",
    "\n",
    "Respuesta: con estos parametros total_variation_weight = 0.5 ,style_weight = 0.5 y content_weight = 10 la trasferencia de estilo es menor y aumenta el ruido en la imagen generada, la transicion es menos suavizada.\n",
    "\n",
    "Resultado con parametros total_variation_weight = 0.1 ,style_weight = 10 y content_weight = 1\n",
    "<img src=\"https://github.com/sabriantonelli/Trabajo_Final_StyleTransfer_SabrinaAntonelli/blob/main/content/output/output_at_iteration_8.png?raw=true\">\n",
    "\n",
    "Resultado con parametros total_variation_weight = 0.5 ,style_weight = 0.5 y content_weight = 10\n",
    "<img src=\"https://github.com/sabriantonelli/Trabajo_Final_StyleTransfer_SabrinaAntonelli/blob/main/content/output8/output_at_iteration_8.png?raw=true\">\n",
    "\n",
    "\n",
    "Imagenes:\n",
    "https://github.com/sabriantonelli/Trabajo_Final_StyleTransfer_SabrinaAntonelli/tree/main/content/output8\n",
    "\n",
    "\n",
    "\n",
    "# 9) Cambiar las imágenes de contenido y estilo por unas elegidas por usted. Adjuntar el resultado.\n",
    "\n",
    "\n",
    "Content image\n",
    "<img src=\"https://github.com/sabriantonelli/Trabajo_Final_StyleTransfer_SabrinaAntonelli/blob/main/content9/IMG_0304.jpeg?raw=true\">\n",
    "\n",
    "Style image\n",
    "<img src=\"https://github.com/sabriantonelli/Trabajo_Final_StyleTransfer_SabrinaAntonelli/blob/main/content9/style.jpeg?raw=true\">\n",
    "\n",
    "Generated image\n",
    "<img src=\"https://github.com/sabriantonelli/Trabajo_Final_StyleTransfer_SabrinaAntonelli/blob/main/content9/output9/output_at_iteration_9.png?raw=true\">\n",
    "\n",
    "\n",
    "Imagenes: https://github.com/sabriantonelli/Trabajo_Final_StyleTransfer_SabrinaAntonelli/tree/main/content9/output9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Trabajo Final CNN - Style Transfer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
